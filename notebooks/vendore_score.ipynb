{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6095036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98824ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vendor_scorecard():\n",
    "    # === STEP 1: Load Data ===\n",
    "    try:\n",
    "        df = pd.read_csv(\"../data/cleaned_message.csv\")  \n",
    "        ner_df = pd.read_csv(\"../data/tokenized_messages.csv\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data files: {e}\")\n",
    "        return None\n",
    "    required_columns = {'ID', 'Date'}\n",
    "    if not required_columns.issubset(df.columns) or not required_columns.issubset(ner_df.columns):\n",
    "        print(\"Missing required columns in input data\")\n",
    "        return None\n",
    "\n",
    "    merged = pd.merge(df, ner_df, on=['ID'], how='left', suffixes=('', '_ner'))\n",
    "    \n",
    "    # Convert and clean date\n",
    "    merged['Date'] = pd.to_datetime(merged['Date'], errors='coerce')\n",
    "    merged = merged.dropna(subset=['Date'])\n",
    "    \n",
    "    if merged.empty:\n",
    "        print(\"No valid data after merging and cleaning\")\n",
    "        return None\n",
    "\n",
    "    # === STEP 3: Compute Metrics Per Vendor ===\n",
    "    vendor_scores = []\n",
    "\n",
    "    for vendor, group in merged.groupby('ID'):\n",
    "        group = group.sort_values('Date')\n",
    "        \n",
    "        # Activity & Consistency\n",
    "        time_span = (group['Date'].max() - group['Date'].min()).days\n",
    "        days_range = time_span if time_span > 0 else 1  # avoid division by zero\n",
    "        posts_per_week = round((len(group) / days_range) * 7, 2)\n",
    "\n",
    "        # Engagement (handle missing views)\n",
    "        avg_views = group['views'].mean() if 'views' in group and not group['views'].isnull().all() else 0\n",
    "        \n",
    "        # Top post info\n",
    "        top_post = group.loc[group['views'].idxmax()] if not group['views'].isnull().all() else {}\n",
    "        top_product = top_post.get('product', 'N/A')\n",
    "        top_price = top_post.get('price', 'N/A')\n",
    "\n",
    "        # Business Profile (price analysis)\n",
    "        prices = pd.to_numeric(group['price'], errors='coerce')\n",
    "        valid_prices = prices.dropna()\n",
    "        avg_price = valid_prices.mean() if not valid_prices.empty else 0\n",
    "\n",
    "        # Normalized scores (0-100 range)\n",
    "        views_score = min(avg_views / 1000 * 100, 100)  # assuming 1000 views = max score\n",
    "        activity_score = min(posts_per_week * 10, 100)   # assuming 10 posts/week = max score\n",
    "        \n",
    "        # Weighted lending score\n",
    "        lending_score = (views_score * 0.6) + (activity_score * 0.4)\n",
    "\n",
    "        vendor_scores.append({\n",
    "            \"Vendor ID\": vendor,\n",
    "            \"Avg. Views/Post\": round(avg_views, 2),\n",
    "            \"Posts/Week\": posts_per_week,\n",
    "            \"Avg. Price (ETB)\": round(avg_price, 2) if avg_price else \"N/A\",\n",
    "            \"Top Product\": top_product,\n",
    "            \"Top Price\": top_price,\n",
    "            \"Lending Score\": round(lending_score, 1)  # one decimal place\n",
    "        })\n",
    "\n",
    "    # === STEP 4: Output Scorecard ===\n",
    "    scorecard_df = pd.DataFrame(vendor_scores)\n",
    "    scorecard_df = scorecard_df.sort_values(\"Lending Score\", ascending=False)\n",
    "    \n",
    "    try:\n",
    "        scorecard_df.to_csv(\"../data/vendor_scorecard.csv\", index=False)\n",
    "        print(\"Successfully generated vendor scorecard\")\n",
    "        return scorecard_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving scorecard: {e}\")\n",
    "        return None\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result = generate_vendor_scorecard()\n",
    "    if result is not None:\n",
    "        print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEWS_WEIGHT = 0.5      # Engagement importance\n",
    "FREQUENCY_WEIGHT = 0.3  # Activity importance\n",
    "PRICE_WEIGHT = 0.2      # Business profile importance\n",
    "\n",
    "def calculate_lending_score(avg_views, posts_per_week, avg_price, max_views=5000, max_freq=20, max_price=10000):\n",
    "    \"\"\"Normalize metrics to 0-100 scale and compute weighted score.\"\"\"\n",
    "    views_norm = min(avg_views / max_views * 100, 100)\n",
    "    freq_norm = min(posts_per_week / max_freq * 100, 100)\n",
    "    price_norm = min(avg_price / max_price * 100, 100) if avg_price > 0 else 0\n",
    "    \n",
    "    return (views_norm * VIEWS_WEIGHT) + (freq_norm * FREQUENCY_WEIGHT) + (price_norm * PRICE_WEIGHT)\n",
    "\n",
    "def generate_scorecard():\n",
    "    # Load data\n",
    "    try:\n",
    "        posts_df = pd.read_csv(\"../data/cleaned_message.csv\")  # Columns: ID, Date, message, views\n",
    "        ner_df = pd.read_csv(\"../data/tokenized_messages.csv\")  # Columns: ID, product, price\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Merge and clean data\n",
    "    merged = pd.merge(posts_df, ner_df, on=\"ID\", how=\"left\")\n",
    "    merged[\"Date\"] = pd.to_datetime(merged[\"Date\"], errors=\"coerce\")\n",
    "    merged = merged.dropna(subset=[\"Date\", \"views\"])\n",
    "    \n",
    "    # Calculate metrics per vendor\n",
    "    metrics = []\n",
    "    for vendor_id, group in merged.groupby(\"ID\"):\n",
    "        group = group.sort_values(\"Date\")\n",
    "        \n",
    "        # Activity & Consistency\n",
    "        days_active = (group[\"Date\"].max() - group[\"Date\"].min()).days\n",
    "        posts_per_week = (len(group) / max(days_active, 1)) * 7  # Prevent division by zero\n",
    "        \n",
    "        # Engagement\n",
    "        avg_views = group[\"views\"].mean()\n",
    "        top_post = group.loc[group[\"views\"].idxmax()]\n",
    "        \n",
    "        # Business Profile\n",
    "        prices = pd.to_numeric(group[\"price\"], errors=\"coerce\").dropna()\n",
    "        avg_price = prices.mean() if not prices.empty else 0\n",
    "        \n",
    "        # Calculate Lending Score\n",
    "        score = calculate_lending_score(avg_views, posts_per_week, avg_price)\n",
    "        \n",
    "        metrics.append({\n",
    "            \"Vendor ID\": vendor_id,\n",
    "            \"Avg. Views/Post\": round(avg_views),\n",
    "            \"Posts/Week\": round(posts_per_week, 1),\n",
    "            \"Avg. Price (ETB)\": round(avg_price) if avg_price > 0 else \"N/A\",\n",
    "            \"Top Product\": top_post.get(\"product\", \"N/A\"),\n",
    "            \"Top Product Views\": top_post[\"views\"],\n",
    "            \"Lending Score\": round(score, 1)  # 0-100 scale\n",
    "        })\n",
    "\n",
    "    # Generate and save scorecard\n",
    "    scorecard = pd.DataFrame(metrics).sort_values(\"Lending Score\", ascending=False)\n",
    "    scorecard.to_csv(\"../data/vendor_scorecard.csv\", index=False)\n",
    "    return scorecard\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    scorecard = generate_scorecard()\n",
    "    print(\"\\n=== Top 5 Vendors ===\")\n",
    "    print(scorecard.head().to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
